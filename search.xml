<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Mysql回顾]]></title>
    <url>%2F2020%2F03%2F13%2FMysql%E5%9B%9E%E9%A1%BE%2F</url>
    <content type="text"><![CDATA[事务：指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。 ACID 1.原子性（Atomicity）要么都成功，要么都失败,即事务是最小单位，不允许切割 eg: A账户给B账户转账，A扣100元钱，B增加100元钱 2.一致性（Consistency） 数据库在事务执行前后都保持一致性状态。在一致性状态下，所有事务对同一个数据的读取结果都是相同的。 3.隔离性（Isolation） 一个事务所做的修改在最终提交以前，对其它事务是不可见的。 4.持久性（Durability） 一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。 并发事务带来的问题： 1.脏读 T1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。 2.丢失修改 T1，T2,读取到某数据20，T1修改20-1，T2修改20-1，最终19，T1修改被丢失。 3.不可重复读 T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。 4.幻读 同不可重复读一样，T2可能增加了几行，T1蒙蔽了，出现了幻觉。 事务的隔离级别 1.READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 2.READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。** 3.REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 explain:在可重复读中，该sql第一次读取到数据后，就将这些数据加锁（悲观锁），其它事务无法修改这些数据，就可以实现可重复读了。但这种方法却无法锁住insert的数据，所以当事务A先前读取了数据，或者修改了全部数据，事务B还是可以insert数据提交，这时事务A就会发现莫名其妙多了一条之前没有的数据，这就是幻读，不能通过行锁来避免。需要Serializable隔离级别 ，读用读锁，写用写锁，读锁和写锁互斥，这么做可以有效的避免幻读、不可重复读、脏读等问题，但会极大的降低数据库的并发能力。 4.SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读） 锁机制与InnoDB锁算法MyISAM和InnoDB存储引擎使用的锁： MyISAM采用表级锁(table-level locking)。 InnoDB支持行级锁(row-level locking)和表级锁,默认为行级锁 表级锁和行级锁对比： 表级锁： MySQL中锁定 粒度最大 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM和 InnoDB引擎都支持表级锁。 行级锁： MySQL中锁定 粒度最小 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁（why?）。 大表优化当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下： 1. 限定数据的范围务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内； 2. 读/写分离经典的数据库拆分方案，主库负责写，从库负责读； 3. 垂直分区根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。 简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。 如下图所示，这样来说大家应该就更容易理解了。 垂直拆分的优点： 可以使得列数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。 垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂； 4. 水平分区保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。 水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。 水平拆分可以支持非常大的数据量。需要注意的一点是：分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 水平拆分最好分库 。 索引Mysql索引主要使用的两种数据结构 哈希索引 O(1) 不适合范围查询。如SELECT * FROM tb1 WHERE id &lt; 500，查500次？ InnoDB不支持哈希索引 对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。 BTree索引 和 B+Tree索引 1.B树的所有节点既存放 键(key) 也存放 数据(data);而B+树只有叶子节点存放 key 和 data，其他内节点只存放key。 2.B树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。 3.B树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显（大大提升范围查询的效率）。 B+树]]></content>
  </entry>
  <entry>
    <title><![CDATA[相册更新]]></title>
    <url>%2F2019%2F12%2F01%2F%E7%9B%B8%E5%86%8C%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[将图片放入photos文件夹下 运行tool.py将相册上传至七牛云photov1 hexo clean hexo d -g 注意：每30天需要将photov1备份到一个新的存储空间，获得30天域名后，修改source/js/src/photo.js的域名]]></content>
  </entry>
  <entry>
    <title><![CDATA[Unet]]></title>
    <url>%2F2019%2F09%2F25%2FUnet%2F</url>
    <content type="text"><![CDATA[论文题目：U-Net: Convolutional Networks for Biomedical Image Segmentation 下载：点击下载 Introduction: 作者提出一个基于FCN改进的U型结构的网络（U-net）和一个依赖strong use of data augmentation的训练策略，可以更充分利用训练样本。 相较于FCN的改进： 1.可以在更少的图片上训练 2.有更精确的分割 与FCN逐点相加不同，U-Net采用将特征在channel维度拼接在一起，形成更“厚”的特征 注：直接复制过来再裁剪到与上采样图片一样大小 该方法允许任意大图片的无缝分割通过一个overlap-tile策略。为了预测框中图像，缺失区域通过镜像输入图像扩张。这种tiling方法对于应用网络到大图像很重要，因为否则结果会被gpu内存限制。为了预测黄色区域的分割，需要蓝色区域作为输入。 数据增强：elastic deformations (弹性形变) 数据增强在训练样本比较少的时候,能够让神经网络学习一些不变性，弹性变换是本文使用的方法。（因为弹性形变是实际细胞中比较常见的一种形变，如果我们能采取数据增强的算法去使网络学习这种形变的不变性，就可以在分割数据集很小的情况下，使网络具有遇见弹性形变还是可以准确的检测出，相当于就是把原图，做了下弹性变形，然后，就相当于扩大了数据集嘛，自然网络就能适应这种弹性变化了，在遇见弹性变形的时候一样可以正确的分类分割） 增加touching cell之间border的权重， 参考： https://blog.csdn.net/jianyuchen23/article/details/79349694]]></content>
  </entry>
  <entry>
    <title><![CDATA[全卷积网络]]></title>
    <url>%2F2019%2F09%2F23%2F%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[论文题目：Fully Convolutional Networks for Semantic Segmentation论文下载: 点击下载 Intorduction: FCN毫无疑问是语义分割领域的经典之作，在FCN出现之前，传统的CNN分割是将像素周围一个小区域作为CNN输入，做训练和预测，这样低效且不准确（忽略整体信息）。FCN主要有三点创新： 卷积化：即将传统CNN结构（文中提到的Alexnet、VGG）最后的全连接层改成卷积层，以便进行直接分割，这是十分有创造性的。这样，使得网络可以接受任意大小的图片，并输出和原图一样大小的分割图。只有这样，才能为每个像素做分类。(像素分类使用图像分类模型（如AlexNet VGGNet等pre-trained model）做迁移学习。) 上采样 or 反卷积：由于网络过程中进行了一系列下采样，使得特征层大小减小，了最后得到的预测层和原图一致，需要采用上采样。 并联跳跃结构：想法类似于resnet和inception，在进行分类预测时利用多层信息。 下图是传统分类CNN和FCN的对比，简单的说，FCN与CNN的区别在于FCN把CNN最后的全连接层换成卷积层，输出一张已经label好的图。 框架如如下，采用了skip connection 上图不够清晰，可以看下图 不同层次对比，其中FCN-8s效果最好 不足：对细节不敏感，没有充分考虑像素之间的关系，缺乏空间一致性。]]></content>
  </entry>
  <entry>
    <title><![CDATA[yolo改进]]></title>
    <url>%2F2019%2F09%2F17%2Fyolo%E6%94%B9%E8%BF%9B%2F</url>
    <content type="text"><![CDATA[论文题目：Assisted Excitation of Activations: A Learning Technique to Improve Object Detectors（CVPR2019）论文下载: 点击下载 摘要：在训练过程中，加入定位信息。可是提升yolov2 map 3.8个点， yolov3 map 2.2个点。这个方法适用于大多数single-stage 目标检测器。只改变了训练过程，推断过程没有任何改变。 Introduction:yolo难以解决得两个痛点：a. difficulty in localization原因：因为yolo同时做分类和定位，最后一层卷积层，更多语义信息，对分类有益。但是spatially course for localization.b. 训练时，前景与背景类别不平衡原因：不同于two-stage 检测器，没有预先减少候选框搜索空间到一个受限制的数目。大多数是简单的负样本。 Related Work: 加入辅助信息到CNN，主要分类两类： 1.同时做检测和分割，提升两个任务的表现。 2.只加入segmentation features来提高检测的精度。 本文提出的方法，在训练检测器时加入weak segmentation ground-truth(即bounding box，从而避免单独引入分割标注，更加简单),并没有增加额外的损失函数。 如上图所示，只在训练时增加了一个Assisted Excitation层。 具体过程： 最终期望的生成特征如下，其中alpha是关于时间的函数用于控制训练中的强度衰减，l+1代表第l+1层，式中c为通道数，e是增强特征： bbox内的像素位置为1，生成一个0-1mask。可见只在bbox内的区域做增强： 增强是按照通道去平均等量加上去的（作者的实验证明该效果最好）： 实验结果： ​ 从上左边的图可以看到，AE强化过的网络有全面的提升，其中在大尺度上的提升更加明显，推测原因是：大物体上加了分割强化后能够获得更强的辨认度，小物体由于本身尺度不大所以增加后也不明显。结果而言印证了这种强化的有效性，但是也完全地陷入了小目标检测的弊端了–像素内容少而被忽视。 ​ 右图的信息不太好辨认。先看yolov2的曲线来说，低iou阈值能够得到更高的改进的精度，说明其召回更好了，但是精度一高就趋于重合，改进失效，说明这种增强提高了低质量bbox的精度。再看yolov3，全IoU都有少量的提高，但是不特别大且没有明显的趋势，说明其采用的多尺度预测能一定程度地解决问题，并在其基础上能对全部精度都有增益。]]></content>
  </entry>
  <entry>
    <title><![CDATA[xml转txt]]></title>
    <url>%2F2019%2F05%2F29%2Fxml%E8%BD%ACtxt%2F</url>
    <content type="text"><![CDATA[先操作一波123456789path = &quot;images/&quot;for filenames in os.walk(pathh): filenames = list(filenames) filenames = filenames[2] for filename in filenames: print(filename) with open (&quot;class_train1.txt&quot;,&apos;a&apos;) as f: f.write(path+filename+&apos;\n&apos;) 再操作一波1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# -*- coding: utf-8 -*-import xml.etree.ElementTree as ETimport pickleimport osfrom os import listdir, getcwdfrom os.path import joinsets = []classes = [&quot;dog&quot;, &quot;person&quot;, &quot;cat&quot;]# 原样保留。size为图片大小# 将ROI的坐标转换为yolo需要的坐标# size是图片的w和h# box里保存的是ROI的坐标（x，y的最大值和最小值）# 返回值为ROI中心点相对于图片大小的比例坐标，和ROI的w、h相对于图片大小的比例def convert(size, box): dw = 1. / (size[0]) dh = 1. / (size[1]) x = (box[0] + box[1]) / 2.0 - 1 y = (box[2] + box[3]) / 2.0 - 1 w = box[1] - box[0] h = box[3] - box[2] x = x * dw w = w * dw y = y * dh h = h * dh return (x, y, w, h)def convert_annotation(image_add): # image_add进来的是带地址的.jpg image_add = os.path.split(image_add)[1] # 截取文件名带后缀 image_add = image_add[0:image_add.find(&apos;.&apos;, 1)] # 删除后缀，现在只有文件名没有后缀 print(image_add) # 现在传进来的只有图片名没有后缀 in_file = open(&apos;xml/&apos; + image_add + &apos;.xml&apos;,encoding=&apos;utf-8&apos;) out_file = open(&apos;hebing2/labels/%s.txt&apos; % (image_add), &apos;w&apos;) tree = ET.parse(in_file) root = tree.getroot() size = root.find(&apos;size&apos;) w = int(size.find(&apos;width&apos;).text) h = int(size.find(&apos;height&apos;).text) # 在一个XML中每个Object的迭代 for obj in root.iter(&apos;object&apos;): # iter()方法可以递归遍历元素/树的所有子元素 # 找到所有的椅子 cls = obj.find(&apos;name&apos;).text # 如果训练标签中的品种不在程序预定品种，或者difficult = 1，跳过此object # cls_id 只等于1 cls_id = 0 xmlbox = obj.find(&apos;bndbox&apos;) # b是每个Object中，一个bndbox上下左右像素的元组 b = (float(xmlbox.find(&apos;xmin&apos;).text), float(xmlbox.find(&apos;xmax&apos;).text), float(xmlbox.find(&apos;ymin&apos;).text), float(xmlbox.find(&apos;ymax&apos;).text)) bb = convert((w, h), b) out_file.write(str(cls_id) + &quot; &quot; + &quot; &quot;.join([str(a) for a in bb]) + &apos;\n&apos;)if not os.path.exists(&apos;hebing2/labels/&apos;): os.makedirs(&apos;hebing2/labels/&apos;)image_adds = open(&quot;class_train1.txt&quot;)for image_add in image_adds: # print(image_add) image_add = image_add.strip() # print (image_add) convert_annotation(image_add) copy from here]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内镜去黑边]]></title>
    <url>%2F2019%2F05%2F09%2F%E5%86%85%E9%95%9C%E5%8E%BB%E9%BB%91%E8%BE%B9%2F</url>
    <content type="text"><![CDATA[对内境图片裁剪，去除黑边，效果如下图所示原图标出矩形框结果123456789101112131415161718192021222324252627282930313233343536373839404142import cv2import osdef main01(): root = &quot;C:\\Users\\liuminggui\\Desktop\\rename\\all\\&quot; # 图片来源路径 save_path = &quot;C:\\Users\\liuminggui\\Desktop\\rename\\save\\&quot; # 图片修改后的保存路径 images = os.listdir(root) for i in images: path = root + i print(i) x,y,w,h=rect_crop(path) # 得到要裁剪的，左上角坐标(x,y)和宽度w,高度h image=cv2.imread(root+i) cv2.imwrite(save_path+i, image[y:y+h+1,x:x+w+1]) # 保存裁剪图片def rect_crop(root=r&apos;F:\Project\DenseBox\data\000001.jpg&apos;): img = cv2.imread(root) # img = cv2.pyrDown(img) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # 得到灰度图 x_,y_ = gray.shape ret,thresh = cv2.threshold(gray,50,255,cv2.THRESH_BINARY) # 测试得到50比较好 # 注意opencv2和3这个函数可能有2个或者三个返回值 img111,contours,hier = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE) for c in contours: # 遍历所有轮廓 print(c) x,y,w,h = cv2.boundingRect(c) if w * h &gt; x_ * y_ * 0.33: # 轮廓大于整个面积的1/3，应该就是我们要找的 cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2) print(x,y,w,h) cv2.imshow(&apos;img+rectangle&apos;,img) cv2.imshow(&apos;thresh&apos;, thresh) cv2.waitKey(0) return (x,y,w,h) return (0,0,0,0) # 找不到大矩形轮廓，则返回默认值0if __name__ == &apos;__main__&apos;: # main1() # liuminggui() main01() cover from YSH and sloan]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日本胃癌论文总结2]]></title>
    <url>%2F2019%2F05%2F07%2F%E6%97%A5%E6%9C%AC%E8%83%83%E7%99%8C%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%932%2F</url>
    <content type="text"><![CDATA[论文题目：Automatic detection of early gastric cancer in endoscopic images using a transferring convolutional neural network 摘要： Accuracy :87.6% heat map accuracy: 82.8%网络：GoogLeNet, 22 conv layers, pretrained on ImageNet 方法： 数据集处理 CNN迁移学习 judge normal vs cancer visualization – heat map 数据集处理（most important）总共有926张分辨率为1000*870的图片其中228包含胃癌 训练数据：从228张选出100张，然后对这100张，每张随机裁剪出100张左右224224的胃癌图片，每张都要包含80%病变区域，得到9587张224224的胃癌图片从包含胃癌和不包含胃癌的图片中随机裁剪出9800张224*224不包含胃癌的正常图片 测试数据：在训练数据裁剪未使用的包含和不包含胃癌的图片中，裁剪出4653胃癌图片和4997正常图片 结果]]></content>
  </entry>
  <entry>
    <title><![CDATA[日本胃癌论文总结1]]></title>
    <url>%2F2019%2F05%2F05%2F%E6%97%A5%E6%9C%AC%E8%83%83%E7%99%8C%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%931%2F</url>
    <content type="text"><![CDATA[论文题目：Application of artificial intelligence using a convolutional neural network for detecting gastric cancer in endoscopic images论文下载: 点击下载 测试集：13584张胃癌图片，包含2639个胃癌病变（经组织学验证）测试集： 2296张胃癌图片，包含69个病人，77个胃癌病变(62 cases had 1 gastric cancer lesion, 6 had 2 lesions, and 1 had 3 lesions)，每个病人18~69张图片。速度： 共用49s 检测2296张图片overall sensitivity： 92.2% （71/77） 71个胃癌病变成功被检测出来positive predictive value： 30.6%=71/（71+161） 161个非癌性病变误检测，过半误检测为胃炎 结果：实验：将训练集resize到300*300,送入网络fine-tune参数，然后检测测试集，检测胃癌病变区域。将其用矩形框框出。 714张图片被诊断出胃癌 714/2639=31.1% 测试集中52个（67.5%）是早期胃癌T1, 25个(32.5%)是advanced cancer T2,T3,T4 平均肿瘤大小是24mm(3到170mm) 思考：准确率该如何计算，如果单看被检测所有的测试集图片，只有不到1/3的图片被检测出有胃癌。但是如果按照检测的胃癌病变，一共有71/77个病变被检测出来！我想了下，主要是因为一个病变包含多张图片，作者认为只要某个病变的一张图片被正确诊断，就认为该病变被成功检测出来。类似于多示例学习。但是这样的话，误诊的也很高，这个161个非癌性病变被误诊是怎么算出来的？？？医生对误诊的区域进行手动分类统计？？？]]></content>
  </entry>
  <entry>
    <title><![CDATA[ObjectDetection]]></title>
    <url>%2F2019%2F04%2F17%2FObjectDetection%2F</url>
    <content type="text"><![CDATA[1.Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.pdf点击下载2.Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf点击下载3.yolov3点击下载]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch]]></title>
    <url>%2F2019%2F04%2F11%2Fpytorch%2F</url>
    <content type="text"><![CDATA[损失函数1. CrossEntropylossa.交叉熵损失函数，常用于分类b.用这个loss前面不需要加 softmax层c.该函数限制了target的类型为torch.LongTensor1234567891011import torch as tfrom torch import nnfrom torch.autograd import Variable as V# batch_size=4, 计算每个类别分数（二分类）output = V(t.randn(4,2)) # batch_size * C=(batch_size, C)# target必须是LongTensor!target =V(t.Tensor([1,0,1,1])).long()criterion = nn.CrossEntropyLoss()loss = criterion(output, target)print(&apos;loss&apos;, loss) output: loss tensor(1.0643) 2. toch.nn.MSELoss均方损失函数，类似于nn.L1Loss函数：1234567import torchloss_fn = torch.nn.MSELoss(reduce=False, size_average=False)input = torch.autograd.Variable(torch.randn(3,4))target = torch.autograd.Variable(torch.randn(3,4))loss = loss_fn(input, target)print(input); print(target); print(loss)print(input.size(), target.size(), loss.size()) output:]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>summary</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用总结]]></title>
    <url>%2F2019%2F04%2F10%2F%E5%B8%B8%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[ROC曲线根据机器学习中分类器的预测得分对样例(每个样例的阳性概率)进行排序，按照顺序逐个把样本的概率作为阈值thresholds进行预测，计算出FPR和TPR。分别以FPR、TPR为横纵坐标作图即可得到ROC曲线。所以作ROC曲线时，需要先求出FPR和TPR。这两个变量的定义：FPR = TP/(TP+FN) TPR = TP/(TP+FP) 将样本输入分类器，每个样本将得到一个预测得分。我们通过设置不同的截断点，即可截取不同的信息。对应此示例图中，每个阈值的识别结果对应一个点(FPR，TPR)。当阈值取最大时，所有样本都被识别成负样本，对应于坐下角的点(0,0); 当阈值取最小时，所有样本都被识别成正样本，对应于右上角的点(1,1)，随着阈值从最大变化到最小，TP和FP都逐渐大；python中调用ROC12345678910111213141516171819import numpy as npfrom sklearn import metricsimport matplotlib.pyplot as pltfrom sklearn.metrics import auc# 真实标签y_true = np.array([0,0,1,1])print(&apos;y_true: &apos;, y_true)# y_score为预测为阳性的得分（说概率不大准确，因为这个score可以大于1）y_score = np.array([0.1, 0.35, 0.3, 0.8])print(&apos;y_score:&apos;, y_score)fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score, pos_label=1)print(&apos;fpr&apos;, fpr)print(&apos;tpr&apos;, tpr)print(&apos;thresholds&apos;, thresholds)plt.plot(fpr,tpr,marker = &apos;o&apos;)plt.show()AUC = auc(fpr, tpr)print(&apos;AUC&apos;, AUC) 输出：阈值[0]表示没有被预测的实例，并且被任意设置为max(y_score) + 1 极大似然估计]]></content>
      <categories>
        <category>一些总结</category>
      </categories>
      <tags>
        <tag>summary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yolo 理论总结]]></title>
    <url>%2F2019%2F04%2F06%2Fyolo%E7%90%86%E8%AE%BA%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[对象识别和定位，可以看成两个任务：找到图片中某个存在对象的区域，然后识别出该区域中具体是哪个对象。 对象识别这件事（一张图片仅包含一个对象，且基本占据图片的整个范围），最近几年基于CNN卷积神经网络的各种方法已经能达到不错的效果了。所以主要需要解决的问题是，对象在哪里。 最简单的想法，就是遍历图片中所有可能的位置，地毯式搜索不同大小，不同宽高比，不同位置的每个区域，逐一检测其中是否存在某个对象，挑选其中概率最大的结果作为输出。显然这种方法效率太低。 RCNN提出候选区(Region Proposals)的方法，先从图片中搜索出一些可能存在对象的候选区（Selective Search），然后对每个候选区进行对象识别。大幅提升了对象识别和定位的效率。总体来说，RCNN系列依然是两阶段处理模式：先提出候选区，再识别候选区中的对象。 yolov1 yolov1详解(非常详细，推荐) 补充：边框回归：对于窗口一般使用四维向量(x,y,w,h)来表示， 分别表示窗口的中心点坐标和宽高。 对于图 2, 红色的框 P 代表原始的Proposal, 绿色的框 G 代表目标的 Ground Truth， 我们的目标是寻找一种关系使得输入原始的窗口 P 经过映射得到一个跟真实窗口 G 更接近的回归窗口G^。 YOLOV1的bounding box并不是Faster RCNN的AnchorFaster RCNN等一些算法采用每个grid中手工设置n个Anchor（先验框，预先设置好位置的bounding box）的设计，每个Anchor有不同的大小和宽高比。YOLO的bounding box看起来很像一个grid中2个Anchor，但它们不是。YOLO并没有预先设置2个bounding box的大小和形状，也没有对每个bounding box分别输出一个对象的预测。它的意思仅仅是对一个对象预测出2个bounding box，选择预测得相对比较准的那个。 Yolov2 yolov2改变：batch normalization,采用了anchor,借鉴Faster RCNN的做法，YOLO2也尝试采用先验框（anchor）。在每个grid预先设定一组不同大小和宽高比的边框，来覆盖整个图像的不同位置和多种尺度，这些先验框作为预定义的候选区在神经网络中将检测其中是否存在对象，以及微调边框的位置。]]></content>
      <categories>
        <category>yolo</category>
      </categories>
      <tags>
        <tag>summary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[斯坦福皮肤癌论文总结]]></title>
    <url>%2F2019%2F03%2F15%2F%E6%96%AF%E5%9D%A6%E7%A6%8F%E7%9A%AE%E8%82%A4%E7%99%8C%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Dermatologist-level classification of skin cancer with deep neural networks背景 以往的皮肤癌分类器往往缺乏好的泛化能力，由于缺少数据和focus on 标准任务，如只对专用医学设备产生的图片进行分类。无法对如手机拍摄的等因为缩放，角度，光线问题的照片进行分类。该文提出一种端对端的CNN，对皮肤癌进行分类。可以达到专家水平甚至更好。 数据 用了129450张图像（比以往的数据集大两个数量级）包含2032种不同的疾病。测试数据是由21位皮肤科专家标注的。 将数据划分： 127,463用于训练和validation 1,942 biopsy-labelled（活检）用于测试 模型GoogLeNet Inception V3 (用2014ImageNet预训练，1.28 million images) 结果蓝色的是CNN,红色的点代表皮肤病专家，绿色的是皮肤病专家的平均水平，可以看出，CNN胜出 在first level nodes（benign lesions, malignant lesions and non-neoplastic lesions)3 class partision 任务中可以达到72.1%的平均准确率，两个皮肤科专家分别达到65.56%和66.0%其次，在second level nodes（9分类）中CNN可以达到55.4%，两个专家分别是53.3和55.0可以看出，用更好的疾病划分方法可以提高准确率 亮点1.一种给疾病分类的算法充分利用如下疾病的树状图分类，好像这个Partition Algorithm 挺好使的看以上结果的时候可以发现，有PA和没有PA，可以提升好几个点，下图是PA具体算法2.本文的训练数据比以往大了两个数量级，数据为王。3.不仅用了专业医学设备产生的图片4.展望手机app端，提升逼格]]></content>
  </entry>
  <entry>
    <title><![CDATA[组会ppt]]></title>
    <url>%2F2019%2F03%2F14%2F%E7%BB%84%E4%BC%9Appt%2F</url>
    <content type="text"><![CDATA[1.2018.3.8 early gastric cancer.ppt点击下载 论文题目：Automatic detection of early gastric cancer in endoscopic images点击下载]]></content>
      <categories>
        <category>PPT</category>
      </categories>
      <tags>
        <tag>PPT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIApaper]]></title>
    <url>%2F2018%2F12%2F10%2FMIApaper%2F</url>
    <content type="text"><![CDATA[1.一种基于原型学习的多示例卷积神经网络点击下载2.Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification点击下载3.2018.12.12小组会PPT点击下载4.自然语言处理paper reading点击下载5.Prototypical Networks for Few-shot Learning点击下载6.Automatic detection of early gastric cancer in endoscopic images点击下载7.what is this点击下载8.Matching Network点击下载9.斯坦福皮肤癌点击下载10.PathologicalEvidenceExplorationinDeepRetinalImageDiagnosis点击下载11.胃癌+AI整理点击下载]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paper]]></title>
    <url>%2F2018%2F11%2F30%2Fpaper%2F</url>
    <content type="text"><![CDATA[paper reading论文下载]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2018%2F11%2F30%2Ftest%2F</url>
    <content type="text"><![CDATA[this is fucking crazy 你好 今天是周五 明天放假了 还有好多作业 this is a test今天是个好日子]]></content>
  </entry>
</search>
