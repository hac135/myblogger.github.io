<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[yolo_改进]]></title>
    <url>%2F2019%2F09%2F02%2Fyolo-%E6%94%B9%E8%BF%9B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[xml转txt]]></title>
    <url>%2F2019%2F05%2F29%2Fxml%E8%BD%ACtxt%2F</url>
    <content type="text"><![CDATA[先操作一波123456789path = &quot;images/&quot;for filenames in os.walk(pathh): filenames = list(filenames) filenames = filenames[2] for filename in filenames: print(filename) with open (&quot;class_train1.txt&quot;,&apos;a&apos;) as f: f.write(path+filename+&apos;\n&apos;) 再操作一波1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# -*- coding: utf-8 -*-import xml.etree.ElementTree as ETimport pickleimport osfrom os import listdir, getcwdfrom os.path import joinsets = []classes = [&quot;dog&quot;, &quot;person&quot;, &quot;cat&quot;]# 原样保留。size为图片大小# 将ROI的坐标转换为yolo需要的坐标# size是图片的w和h# box里保存的是ROI的坐标（x，y的最大值和最小值）# 返回值为ROI中心点相对于图片大小的比例坐标，和ROI的w、h相对于图片大小的比例def convert(size, box): dw = 1. / (size[0]) dh = 1. / (size[1]) x = (box[0] + box[1]) / 2.0 - 1 y = (box[2] + box[3]) / 2.0 - 1 w = box[1] - box[0] h = box[3] - box[2] x = x * dw w = w * dw y = y * dh h = h * dh return (x, y, w, h)def convert_annotation(image_add): # image_add进来的是带地址的.jpg image_add = os.path.split(image_add)[1] # 截取文件名带后缀 image_add = image_add[0:image_add.find(&apos;.&apos;, 1)] # 删除后缀，现在只有文件名没有后缀 print(image_add) # 现在传进来的只有图片名没有后缀 in_file = open(&apos;xml/&apos; + image_add + &apos;.xml&apos;,encoding=&apos;utf-8&apos;) out_file = open(&apos;hebing2/labels/%s.txt&apos; % (image_add), &apos;w&apos;) tree = ET.parse(in_file) root = tree.getroot() size = root.find(&apos;size&apos;) w = int(size.find(&apos;width&apos;).text) h = int(size.find(&apos;height&apos;).text) # 在一个XML中每个Object的迭代 for obj in root.iter(&apos;object&apos;): # iter()方法可以递归遍历元素/树的所有子元素 # 找到所有的椅子 cls = obj.find(&apos;name&apos;).text # 如果训练标签中的品种不在程序预定品种，或者difficult = 1，跳过此object # cls_id 只等于1 cls_id = 0 xmlbox = obj.find(&apos;bndbox&apos;) # b是每个Object中，一个bndbox上下左右像素的元组 b = (float(xmlbox.find(&apos;xmin&apos;).text), float(xmlbox.find(&apos;xmax&apos;).text), float(xmlbox.find(&apos;ymin&apos;).text), float(xmlbox.find(&apos;ymax&apos;).text)) bb = convert((w, h), b) out_file.write(str(cls_id) + &quot; &quot; + &quot; &quot;.join([str(a) for a in bb]) + &apos;\n&apos;)if not os.path.exists(&apos;hebing2/labels/&apos;): os.makedirs(&apos;hebing2/labels/&apos;)image_adds = open(&quot;class_train1.txt&quot;)for image_add in image_adds: # print(image_add) image_add = image_add.strip() # print (image_add) convert_annotation(image_add) copy from here]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内镜去黑边]]></title>
    <url>%2F2019%2F05%2F09%2F%E5%86%85%E9%95%9C%E5%8E%BB%E9%BB%91%E8%BE%B9%2F</url>
    <content type="text"><![CDATA[对内境图片裁剪，去除黑边，效果如下图所示原图标出矩形框结果123456789101112131415161718192021222324252627282930313233343536373839404142import cv2import osdef main01(): root = &quot;C:\\Users\\liuminggui\\Desktop\\rename\\all\\&quot; # 图片来源路径 save_path = &quot;C:\\Users\\liuminggui\\Desktop\\rename\\save\\&quot; # 图片修改后的保存路径 images = os.listdir(root) for i in images: path = root + i print(i) x,y,w,h=rect_crop(path) # 得到要裁剪的，左上角坐标(x,y)和宽度w,高度h image=cv2.imread(root+i) cv2.imwrite(save_path+i, image[y:y+h+1,x:x+w+1]) # 保存裁剪图片def rect_crop(root=r&apos;F:\Project\DenseBox\data\000001.jpg&apos;): img = cv2.imread(root) # img = cv2.pyrDown(img) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # 得到灰度图 x_,y_ = gray.shape ret,thresh = cv2.threshold(gray,50,255,cv2.THRESH_BINARY) # 测试得到50比较好 # 注意opencv2和3这个函数可能有2个或者三个返回值 img111,contours,hier = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE) for c in contours: # 遍历所有轮廓 print(c) x,y,w,h = cv2.boundingRect(c) if w * h &gt; x_ * y_ * 0.33: # 轮廓大于整个面积的1/3，应该就是我们要找的 cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2) print(x,y,w,h) cv2.imshow(&apos;img+rectangle&apos;,img) cv2.imshow(&apos;thresh&apos;, thresh) cv2.waitKey(0) return (x,y,w,h) return (0,0,0,0) # 找不到大矩形轮廓，则返回默认值0if __name__ == &apos;__main__&apos;: # main1() # liuminggui() main01() cover from YSH and sloan]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日本胃癌论文总结2]]></title>
    <url>%2F2019%2F05%2F07%2F%E6%97%A5%E6%9C%AC%E8%83%83%E7%99%8C%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%932%2F</url>
    <content type="text"><![CDATA[论文题目：Automatic detection of early gastric cancer in endoscopic images using a transferring convolutional neural network 摘要： Accuracy :87.6% heat map accuracy: 82.8%网络：GoogLeNet, 22 conv layers, pretrained on ImageNet 方法： 数据集处理 CNN迁移学习 judge normal vs cancer visualization – heat map 数据集处理（most important）总共有926张分辨率为1000*870的图片其中228包含胃癌 训练数据：从228张选出100张，然后对这100张，每张随机裁剪出100张左右224224的胃癌图片，每张都要包含80%病变区域，得到9587张224224的胃癌图片从包含胃癌和不包含胃癌的图片中随机裁剪出9800张224*224不包含胃癌的正常图片 测试数据：在训练数据裁剪未使用的包含和不包含胃癌的图片中，裁剪出4653胃癌图片和4997正常图片 结果]]></content>
  </entry>
  <entry>
    <title><![CDATA[日本胃癌论文总结1]]></title>
    <url>%2F2019%2F05%2F05%2F%E6%97%A5%E6%9C%AC%E8%83%83%E7%99%8C%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%931%2F</url>
    <content type="text"><![CDATA[论文题目：Application of artificial intelligence using a convolutional neural network for detecting gastric cancer in endoscopic images论文下载: 点击下载 测试集：13584张胃癌图片，包含2639个胃癌病变（经组织学验证）测试集： 2296张胃癌图片，包含69个病人，77个胃癌病变(62 cases had 1 gastric cancer lesion, 6 had 2 lesions, and 1 had 3 lesions)，每个病人18~69张图片。速度： 共用49s 检测2296张图片overall sensitivity： 92.2% （71/77） 71个胃癌病变成功被检测出来positive predictive value： 30.6%=71/（71+161） 161个非癌性病变误检测，过半误检测为胃炎 结果：实验：将训练集resize到300*300,送入网络fine-tune参数，然后检测测试集，检测胃癌病变区域。将其用矩形框框出。 714张图片被诊断出胃癌 714/2639=31.1% 测试集中52个（67.5%）是早期胃癌T1, 25个(32.5%)是advanced cancer T2,T3,T4 平均肿瘤大小是24mm(3到170mm) 思考：准确率该如何计算，如果单看被检测所有的测试集图片，只有不到1/3的图片被检测出有胃癌。但是如果按照检测的胃癌病变，一共有71/77个病变被检测出来！我想了下，主要是因为一个病变包含多张图片，作者认为只要某个病变的一张图片被正确诊断，就认为该病变被成功检测出来。类似于多示例学习。但是这样的话，误诊的也很高，这个161个非癌性病变被误诊是怎么算出来的？？？医生对误诊的区域进行手动分类统计？？？]]></content>
  </entry>
  <entry>
    <title><![CDATA[ObjectDetection]]></title>
    <url>%2F2019%2F04%2F17%2FObjectDetection%2F</url>
    <content type="text"><![CDATA[1.Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.pdf点击下载2.Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf点击下载3.yolov3点击下载]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch]]></title>
    <url>%2F2019%2F04%2F11%2Fpytorch%2F</url>
    <content type="text"><![CDATA[损失函数1. CrossEntropylossa.交叉熵损失函数，常用于分类b.用这个loss前面不需要加 softmax层c.该函数限制了target的类型为torch.LongTensor1234567891011import torch as tfrom torch import nnfrom torch.autograd import Variable as V# batch_size=4, 计算每个类别分数（二分类）output = V(t.randn(4,2)) # batch_size * C=(batch_size, C)# target必须是LongTensor!target =V(t.Tensor([1,0,1,1])).long()criterion = nn.CrossEntropyLoss()loss = criterion(output, target)print(&apos;loss&apos;, loss) output: loss tensor(1.0643) 2. toch.nn.MSELoss均方损失函数，类似于nn.L1Loss函数：1234567import torchloss_fn = torch.nn.MSELoss(reduce=False, size_average=False)input = torch.autograd.Variable(torch.randn(3,4))target = torch.autograd.Variable(torch.randn(3,4))loss = loss_fn(input, target)print(input); print(target); print(loss)print(input.size(), target.size(), loss.size()) output:]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>summary</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用总结]]></title>
    <url>%2F2019%2F04%2F10%2F%E5%B8%B8%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[ROC曲线根据机器学习中分类器的预测得分对样例(每个样例的阳性概率)进行排序，按照顺序逐个把样本的概率作为阈值thresholds进行预测，计算出FPR和TPR。分别以FPR、TPR为横纵坐标作图即可得到ROC曲线。所以作ROC曲线时，需要先求出FPR和TPR。这两个变量的定义：FPR = TP/(TP+FN) TPR = TP/(TP+FP) 将样本输入分类器，每个样本将得到一个预测得分。我们通过设置不同的截断点，即可截取不同的信息。对应此示例图中，每个阈值的识别结果对应一个点(FPR，TPR)。当阈值取最大时，所有样本都被识别成负样本，对应于坐下角的点(0,0); 当阈值取最小时，所有样本都被识别成正样本，对应于右上角的点(1,1)，随着阈值从最大变化到最小，TP和FP都逐渐大；python中调用ROC12345678910111213141516171819import numpy as npfrom sklearn import metricsimport matplotlib.pyplot as pltfrom sklearn.metrics import auc# 真实标签y_true = np.array([0,0,1,1])print(&apos;y_true: &apos;, y_true)# y_score为预测为阳性的得分（说概率不大准确，因为这个score可以大于1）y_score = np.array([0.1, 0.35, 0.3, 0.8])print(&apos;y_score:&apos;, y_score)fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score, pos_label=1)print(&apos;fpr&apos;, fpr)print(&apos;tpr&apos;, tpr)print(&apos;thresholds&apos;, thresholds)plt.plot(fpr,tpr,marker = &apos;o&apos;)plt.show()AUC = auc(fpr, tpr)print(&apos;AUC&apos;, AUC) 输出：阈值[0]表示没有被预测的实例，并且被任意设置为max(y_score) + 1 极大似然估计]]></content>
      <categories>
        <category>一些总结</category>
      </categories>
      <tags>
        <tag>summary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yolo 理论总结]]></title>
    <url>%2F2019%2F04%2F06%2Fyolo%E7%90%86%E8%AE%BA%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[对象识别和定位，可以看成两个任务：找到图片中某个存在对象的区域，然后识别出该区域中具体是哪个对象。 对象识别这件事（一张图片仅包含一个对象，且基本占据图片的整个范围），最近几年基于CNN卷积神经网络的各种方法已经能达到不错的效果了。所以主要需要解决的问题是，对象在哪里。 最简单的想法，就是遍历图片中所有可能的位置，地毯式搜索不同大小，不同宽高比，不同位置的每个区域，逐一检测其中是否存在某个对象，挑选其中概率最大的结果作为输出。显然这种方法效率太低。 RCNN提出候选区(Region Proposals)的方法，先从图片中搜索出一些可能存在对象的候选区（Selective Search），然后对每个候选区进行对象识别。大幅提升了对象识别和定位的效率。总体来说，RCNN系列依然是两阶段处理模式：先提出候选区，再识别候选区中的对象。 yolov1 yolov1详解(非常详细，推荐) 补充：边框回归：对于窗口一般使用四维向量(x,y,w,h)来表示， 分别表示窗口的中心点坐标和宽高。 对于图 2, 红色的框 P 代表原始的Proposal, 绿色的框 G 代表目标的 Ground Truth， 我们的目标是寻找一种关系使得输入原始的窗口 P 经过映射得到一个跟真实窗口 G 更接近的回归窗口G^。 YOLOV1的bounding box并不是Faster RCNN的AnchorFaster RCNN等一些算法采用每个grid中手工设置n个Anchor（先验框，预先设置好位置的bounding box）的设计，每个Anchor有不同的大小和宽高比。YOLO的bounding box看起来很像一个grid中2个Anchor，但它们不是。YOLO并没有预先设置2个bounding box的大小和形状，也没有对每个bounding box分别输出一个对象的预测。它的意思仅仅是对一个对象预测出2个bounding box，选择预测得相对比较准的那个。 Yolov2 yolov2改变：batch normalization,采用了anchor,借鉴Faster RCNN的做法，YOLO2也尝试采用先验框（anchor）。在每个grid预先设定一组不同大小和宽高比的边框，来覆盖整个图像的不同位置和多种尺度，这些先验框作为预定义的候选区在神经网络中将检测其中是否存在对象，以及微调边框的位置。]]></content>
      <categories>
        <category>yolo</category>
      </categories>
      <tags>
        <tag>summary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[斯坦福皮肤癌论文总结]]></title>
    <url>%2F2019%2F03%2F15%2F%E6%96%AF%E5%9D%A6%E7%A6%8F%E7%9A%AE%E8%82%A4%E7%99%8C%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Dermatologist-level classification of skin cancer with deep neural networks背景 以往的皮肤癌分类器往往缺乏好的泛化能力，由于缺少数据和focus on 标准任务，如只对专用医学设备产生的图片进行分类。无法对如手机拍摄的等因为缩放，角度，光线问题的照片进行分类。该文提出一种端对端的CNN，对皮肤癌进行分类。可以达到专家水平甚至更好。 数据 用了129450张图像（比以往的数据集大两个数量级）包含2032种不同的疾病。测试数据是由21位皮肤科专家标注的。 将数据划分： 127,463用于训练和validation 1,942 biopsy-labelled（活检）用于测试 模型GoogLeNet Inception V3 (用2014ImageNet预训练，1.28 million images) 结果蓝色的是CNN,红色的点代表皮肤病专家，绿色的是皮肤病专家的平均水平，可以看出，CNN胜出 在first level nodes（benign lesions, malignant lesions and non-neoplastic lesions)3 class partision 任务中可以达到72.1%的平均准确率，两个皮肤科专家分别达到65.56%和66.0%其次，在second level nodes（9分类）中CNN可以达到55.4%，两个专家分别是53.3和55.0可以看出，用更好的疾病划分方法可以提高准确率 亮点1.一种给疾病分类的算法充分利用如下疾病的树状图分类，好像这个Partition Algorithm 挺好使的看以上结果的时候可以发现，有PA和没有PA，可以提升好几个点，下图是PA具体算法2.本文的训练数据比以往大了两个数量级，数据为王。3.不仅用了专业医学设备产生的图片4.展望手机app端，提升逼格]]></content>
  </entry>
  <entry>
    <title><![CDATA[组会ppt]]></title>
    <url>%2F2019%2F03%2F14%2F%E7%BB%84%E4%BC%9Appt%2F</url>
    <content type="text"><![CDATA[1.2018.3.8 early gastric cancer.ppt点击下载 论文题目：Automatic detection of early gastric cancer in endoscopic images点击下载]]></content>
      <categories>
        <category>PPT</category>
      </categories>
      <tags>
        <tag>PPT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIApaper]]></title>
    <url>%2F2018%2F12%2F10%2FMIApaper%2F</url>
    <content type="text"><![CDATA[1.一种基于原型学习的多示例卷积神经网络点击下载2.Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification点击下载3.2018.12.12小组会PPT点击下载4.自然语言处理paper reading点击下载5.Prototypical Networks for Few-shot Learning点击下载6.Automatic detection of early gastric cancer in endoscopic images点击下载7.what is this点击下载8.Matching Network点击下载9.斯坦福皮肤癌点击下载10.PathologicalEvidenceExplorationinDeepRetinalImageDiagnosis点击下载11.胃癌+AI整理点击下载]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paper]]></title>
    <url>%2F2018%2F11%2F30%2Fpaper%2F</url>
    <content type="text"><![CDATA[paper reading论文下载]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2018%2F11%2F30%2Ftest%2F</url>
    <content type="text"><![CDATA[this is fucking crazy 你好 今天是周五 明天放假了 还有好多作业 this is a test今天是个好日子]]></content>
  </entry>
</search>
