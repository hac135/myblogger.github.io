<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Redis]]></title>
    <url>%2F2020%2F03%2F21%2FRedis%2F</url>
    <content type="text"><![CDATA[NoSQL Redis简介 Redis 是完全开源免费的，遵守BSD协议，是一个高性能的key-value数据库。 Redis 与其他 key - value 缓存产品有以下三个特点： Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 Redis与其他key-value存储有什么不同？ Redis有着更为复杂的数据结构并且提供对他们的原子性操作，这是一个不同于其他数据库的进化路径。Redis的数据类型都是基于基本数据结构的同时对程序员透明，无需进行额外的抽象。 Redis运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时需要权衡内存，因为数据量不能大于硬件内存。在内存数据库方面的另一个优点是，相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样Redis可以做很多内部复杂性很强的事情。同时，在磁盘格式方面他们是紧凑的以追加的方式产生的，因为他们并不需要进行随机访问。 redis 和 memcached 的区别对于 redis 和 memcached 我总结了下面四点。现在公司一般都是用 redis 来实现缓存，而且 redis 自身也越来越强大了！ redis支持更丰富的数据类型（支持更复杂的应用场景）：Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。memcache支持简单的数据类型，String。 Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而Memecache把数据全部存在内存之中。 集群模式：memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 redis 目前是原生支持 cluster 模式的. Memcached是多线程，非阻塞IO复用的网络模型；Redis使用单线程的多路 IO 复用模型。 数据类型 String, Hash, List, Set, Zset, Bitmap, HyperLogLog, Geospatial 理解：redis 都是数据结构外面再套一个key, 所以String 就是我们常见的key -val普通形式 redis 过期策略 redis 过期策略是：定期删除+惰性删除。所谓定期删除，指的是 redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。 假设 redis 里放了 10w 个 key，都设置了过期时间，你每隔几百毫秒，就检查 10w 个 key，那 redis 基本上就死了，cpu 负载会很高的，消耗在你的检查过期 key 上了。注意，这里可不是每隔 100ms 就遍历所有的设置过期时间的 key，那样就是一场性能上的灾难。实际上 redis 是每隔 100ms 随机抽取一些 key 来检查和删除的。 但是问题是，定期删除可能会导致很多过期 key 到了时间并没有被删除掉，那咋整呢？所以就是惰性删除了。这就是说，在你获取某个 key 的时候，redis 会检查一下 ，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除，不会给你返回任何东西。 获取 key 的时候，如果此时 key 已经过期，就删除，不会返回任何东西。 但是实际上这还是有问题的，如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期 key 堆积在内存里，导致 redis 内存块耗尽了，咋整？答案是：走内存淘汰机制。 no-eviction：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key（这个是最常用的） volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 4.0版本后增加以下两种： volatile-lfu：从已设置过期时间的数据集(server.db[i].expires)中挑选最不经常使用的数据淘汰 allkeys-lfu：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的key 12A~~A~~A~~A~~A~~A~~A~~A~~A~~A~~~|B~~~~~B~~~~~B~~~~~B~~~~~~~~~~~B| 考虑上面的情况，如果在|处删除，那么A距离的时间最久，但实际上A的使用频率要比B频繁，所以合理的淘汰策略应该是淘汰B。LFU就是为应对这种情况而生的。 redis 持久化机制(怎么保证 redis 挂掉之后再重启数据可以进行恢复) 很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器、机器故障之后恢复数据），或者是为了防止系统故障而将数据备份到一个远程位置。 Redis不同于Memcached的很重一点就是，Redis支持持久化，而且支持两种不同的持久化操作。Redis的一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file,AOF）。这两种方法各有千秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方法。 快照（snapshotting）持久化（RDB） Redis可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性能），还可以将快照留在原地以便重启服务器的时候使用。与AOF相比，在恢复大的数据集时会更快。 快照持久化是Redis默认采用的持久化方式，在redis.conf配置文件中默认有此下配置： 12345save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。save 60 10000#在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 AOF（append-only file）持久化 以日志的形式记录写操作。 与快照持久化相比，AOF持久化 的实时性更好，因此已成为主流的持久化方案。默认情况下Redis没有开启AOF（append only file）方式的持久化，可以通过appendonly参数开启： 1appendonly yes 开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof。 在Redis的配置文件中存在三种不同的 AOF 持久化方式，它们分别是： 123appendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度appendfsync everysec #每秒钟同步一次，显示地将多个写命令同步到硬盘appendfsync no #让操作系统决定何时进行同步 为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec选项 ，让Redis每秒同步一次AOF文件，Redis性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。 开启了RDB时，也可以同时开启AOF，但是恢复的时候会从AOF恢复。 优缺点对比： 1.对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大，恢复速度慢； 2.RDB与AOF相比，在恢复大的数据集时会更快。 3.AOF可以更好的保护数据不丢失。RDB可能会丢失崩溃到上一次快照的数据。 4.RDB在保存的时候需要复制一个副本。如果比较大的话，占内存。 Redis 4.0 对于持久化机制的优化 Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。 如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。 补充内容：AOF 重写 显然，.aof文件会越来越大，文件大小当达到某个值的时候，触发AOF重写。从Redis数据库遍历数据，得到set命令。重写得到新的aof文件。 AOF重写可以产生一个新的AOF文件，这个新的AOF文件和原有的AOF文件所保存的数据库状态一样，但体积更小。 AOF重写是一个有歧义的名字，该功能是通过读取数据库中的键值对来实现的，程序无须对现有AOF文件进行任何读入、分析或者写入操作。 Redis事务 DISCARD 取消事务，放弃执行事务块内的所有命令。EXEC 执行所有事务块内的命令。MULTI 标记一个事务块的开始。UNWATCH 取消 WATCH 命令对所有 key 的监视。WATCH key [key …]监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断。 在redis中，对于一个存在问题的命令，如果在入队的时候就已经出错，整个事务内的命令将都不会被执行（其后续的命令依然可以入队），如果这个错误命令在入队的时候并没有报错，而是在执行的时候出错了，那么redis默认跳过这个命令执行后续命令，不会回滚（没有原子性）。 Redis的集群模式 主从模式 所有的写请求都被发送到主数据库。在由主数据库将数据同步到从数据库上。从数据库主要用于执行读操作缓解系统的读压力。 哨兵模式 哨兵模式是一种特殊的模式，首先Redis提供了哨兵的命令，哨兵是一个独立的进程，作为进程，它会独立运行。其原理是哨兵通过发送命令，等待Redis服务器响应，从而监控运行的多个Redis实例。 这里的哨兵有两个作用 通过发送命令，让Redis服务器返回监控其运行状态，包括主服务器和从服务器。 当哨兵监测到master宕机，会自动将slave切换成master，然后通过发布订阅模式通知其他的从服务器，修改配置文件，让它们切换主机。 然而一个哨兵进程对Redis服务器进行监控，可能会出现问题，为此，我们可以使用多个哨兵进行监控。各个哨兵之间还会进行监控，这样就形成了多哨兵模式。 集群模式 哨兵模式基本可以满足一般生产的需求，具备高可用性。但是当数据量过大到一台服务器存放不下的情况时，主从模式或哨兵模式就不能满足需求了，这个时候需要对存储的数据进行分片，将数据存储到多个Redis实例中。cluster模式的出现就是为了解决单机Redis容量有限的问题，将Redis的数据根据一定的规则分配到多台机器。 cluster可以说是sentinel和主从模式的结合体，通过cluster可以实现主从和master重选功能，所以如果配置两个副本三个分片的话，就需要六个Redis实例。因为Redis的数据是根据一定规则分配到cluster的不同机器的，当数据量过大时，可以新增机器进行扩容。 集群模式特点： 所有的节点都是一主一从（也可以是一主多从），其中从不提供服务，仅作为备用 不支持同时处理多个key（如MSET/MGET），因为redis需要把key均匀分布在各个节点上，并发量很高的情况下同时创建key-value会降低性能并导致不可预测的行为 支持在线增加、删除节点 客户端可以连接任何一个主节点进行读写执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断。 缓存雪崩和缓存穿透问题解决方案 什么是缓存雪崩？ 简介：缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。 eg:对于系统 A，假设每天高峰期每秒 5000 个请求，本来缓存在高峰期可以扛住每秒 4000 个请求，但是缓存机器意外发生了全盘宕机。缓存挂了，此时 1 秒 5000 个请求全部落数据库，数据库必然扛不住，它会报一下警，然后就挂了。此时，如果没有采用什么特别的方案来处理这个故障，DBA 很着急，重启数据库，但是数据库立马又被新的流量给打死了。这就是缓存雪崩。 有哪些解决办法？ 事前：尽量保证整个 redis 集群的高可用性，发现机器宕机尽快补上。选择合适的内存淘汰策略。 事中：本地ehcache缓存 + hystrix限流&amp;降级，避免MySQL崩掉 事后：利用 redis 持久化机制保存的数据尽快恢复缓存 什么是缓存穿透？ 缓存穿透说简单点就是大量请求的 key 根本不存在于缓存中，导致请求直接到了数据库上，根本没有经过缓存这一层。举个例子：某个黑客故意制造我们缓存中不存在的 key 发起大量请求，导致大量请求落到数据库。 对于系统A，假设一秒 5000 个请求，结果其中 4000 个请求是黑客发出的恶意攻击。 黑客发出的那 4000 个攻击，缓存中查不到，每次你去数据库里查，也查不到。 举个栗子。数据库 id 是从 1 开始的，结果黑客发过来的请求 id 全部都是负数。这样的话，缓存中不会有，请求每次都“视缓存于无物”，直接查询数据库。这种恶意攻击场景的缓存穿透就会直接把数据库给打死。 有哪些解决办法？ 最基本的就是首先做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等。 缓存无效 key : 如果缓存和数据库都查不到某个 key 的数据就写一个到 redis 中(可设置为空值)去并设置过期时间，具体命令如下：SET key value EX 10086。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求key，会导致 redis 中缓存大量无效的 key 。很明显，这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，尽量将无效的 key 的过期时间设置短一点比如 1 分钟。 布隆过滤器 布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在与海量数据中。我们需要的就是判断 key 是否合法，有没有感觉布隆过滤器就是我们想要找的那个“人”。具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，我会先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走redis流程。 缓存击穿 缓存击穿，就是说某个 key 非常热点，访问非常频繁，处于集中式高并发访问的情况，当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库，就像是在一道屏障上凿开了一个洞。 解决方式也很简单，可以将热点数据设置为永远不过期；或者基于 redis or zookeeper 实现互斥锁，等待第一个请求构建完缓存之后，再释放锁，进而其它请求才能通过该 key 访问数据（走redis）。 如何解决 Redis 的并发竞争 Key 问题所谓 Redis 的并发竞争 Key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同！ 推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（如果不存在 Redis 的并发竞争 Key 问题，不要使用分布式锁，这样会影响性能） 缓存淘汰策略 在缓存数据过多时需要使用某种淘汰算法决定淘汰哪些数据。常用的有以下几种。 FIFO(First In First Out) : 判断被存储的时间，离目前最远的数据优先被淘汰。 LRU(Least Recently Used) : 判断缓存最近被使用的时间，距离当前时间最远的数据优先被淘汰。 LFU(Least Frequently Used) : 在一段时间内，被使用次数最少的缓存优先被淘汰。]]></content>
  </entry>
  <entry>
    <title><![CDATA[JVM垃圾回收]]></title>
    <url>%2F2020%2F03%2F20%2FJVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%2F</url>
    <content type="text"><![CDATA[揭开 JVM 内存分配与回收的神秘面纱Java 的自动内存管理主要是针对对象内存的回收和对象内存的分配。同时，Java 自动内存管理最核心的功能是 堆 内存中对象的分配与回收。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 堆空间的基本结构 上图所示的 eden 区、s0(“From”) 区、s1(“To”) 区都属于新生代，tentired 区属于老年代。大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s1(“To”)，并且对象的年龄还会加 1(Eden 区-&gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。经过这次GC后，Eden区和”From”区已经被清空。这个时候，”From”和”To”会交换他们的角色，也就是新的”To”就是上次GC前的“From”，新的”From”就是上次GC前的”To”。不管怎样，都会保证名为To的Survivor区域是空的。Minor GC会一直重复这样的过程，直到“To”区被填满，”To”区被填满之后，会将所有对象移动到老年代中。 对象优先在 eden 区分配 目前主流的垃圾收集器都会采用分代回收算法，因此需要将堆内存分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 大多数情况下，对象在新生代中 eden 区分配。当 eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.下面我们来进行实际测试以下。 在测试之前我们先来看看 Minor GC 和 Full GC 有什么不同呢？ 新生代 GC（Minor GC）:指发生新生代的的垃圾收集动作，Minor GC 非常频繁，回收速度一般也比较快。 老年代 GC（Major GC/Full GC）:指发生在老年代的 GC，出现了 Major GC 经常会伴随至少一次的 Minor GC（并非绝对），Major GC 的速度一般会比 Minor GC 的慢 10 倍以上。 大对象直接进入老年代 大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。 为什么要这样呢？ 为了避免为大对象分配内存时由于分配担保机制（Eden不够，提前把新生代的对象转移到老年代中去）带来的复制而降低效率。 长期存活的对象将进入老年代 既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。 如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并将对象年龄设为 1.对象在 Survivor 中每熬过一次 MinorGC,年龄就增加 1 岁，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区-&gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 修正：“Hotspot遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了survivor区的一半时，取这个年龄和MaxTenuringThreshold中更小的一个值，作为新的晋升年龄阈值”。 对象已经死亡？ 堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断那些对象已经死亡（即不能再被任何途径使用的对象）。 引用计数法 给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加 1；当引用失效，计数器就减 1；任何时候计数器为 0 的对象就是不可能再被使用的。 这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。 所谓对象之间的相互引用问题，如下面代码所示：除了对象 objA 和 objB 相互引用着对方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为 0，于是引用计数算法无法通知 GC 回收器回收他们。 123456789101112public class ReferenceCountingGc &#123; Object instance = null; public static void main(String[] args) &#123; ReferenceCountingGc objA = new ReferenceCountingGc(); ReferenceCountingGc objB = new ReferenceCountingGc(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; &#125;&#125; 可达性分析算法 这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。 再谈引用 略 不可达的对象并非“非死不可” 即使在可达性分析法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑阶段”，要真正宣告一个对象死亡，至少要经历两次标记过程；可达性分析法中不可达的对象被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行 finalize 方法。当对象没有覆盖 finalize 方法，或 finalize 方法已经被虚拟机调用过时，虚拟机将这两种情况视为没有必要执行。 被判定为需要执行的对象将会被放在一个队列中进行第二次标记，除非这个对象与引用链上的任何一个对象建立关联，否则就会被真的回收。 如何判断一个常量是废弃常量 运行时常量池主要回收的是废弃的常量。那么，我们如何判断一个常量是废弃常量呢？ 假如在常量池中存在字符串 “abc”，如果当前没有任何 String 对象引用该字符串常量的话，就说明常量 “abc” 就是废弃常量，如果这时发生内存回收的话而且有必要的话，”abc” 就会被系统清理出常量池。 如何判断一个类是无用的类 方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？ 判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面 3 个条件才能算是 “无用的类” ： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述 3 个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。 垃圾收集算法 标记-清除算法 该算法分为“标记”和“清除”阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它是最基础的收集算法，后续的算法都是对其不足进行改进得到。这种垃圾收集算法会带来两个明显的问题： 效率问题 空间问题（标记清除后会产生大量不连续的碎片） 复制算法 为了解决效率问题，“复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 标记-整理算法 根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。 分代收集算法 当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将 java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 比如在新生代中，每次收集都会有大量对象死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。 延伸面试问题： HotSpot 为什么要分为新生代和老年代？ 根据上面的对分代收集算法的介绍回答。 垃圾收集器 如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 虽然我们对各个收集器进行比较，但并非要挑选出一个最好的收集器。因为直到现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，我们能做的就是根据具体应用场景选择适合自己的垃圾收集器。试想一下：如果有一种四海之内、任何场景下都适用的完美收集器存在，那么我们的 HotSpot 虚拟机就不会实现那么多不同的垃圾收集器了。 Serial 收集器 Serial（串行）收集器收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ “Stop The World” ），直到它收集结束。 虚拟机的设计者们当然知道 Stop The World 带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。 但是 Serial 收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。Serial 收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial 收集器对于运行在 Client 模式下的虚拟机来说是个不错的选择。 ParNew 收集器 ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。 它是许多运行在 Server 模式下的虚拟机的首要选择，除了 Serial 收集器外，只有它能与 CMS 收集器（真正意义上的并发收集器，后面会介绍到）配合工作。 并行和并发概念补充： 并行（Parallel） ：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个 CPU 上。 Parallel Scavenge 收集器 Parallel Scavenge 收集器也是使用复制算法的多线程收集器，它看上去几乎和ParNew都一样。 那么它有什么特别之处呢？ 1234567-XX:+UseParallelGC 使用 Parallel 收集器+ 老年代串行-XX:+UseParallelOldGC 使用 Parallel 收集器+ 老年代并行 Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。 Parallel Scavenge 收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，手工优化存在困难的话可以选择把内存管理优化交给虚拟机去完成也是一个不错的选择。 CMS 收集器 CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。 CMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 从名字中的Mark Sweep这两个词可以看出，CMS 收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤： 初始标记： 暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快 ； 并发标记： 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短 并发清除： 开启用户线程，同时 GC 线程开始对为标记的区域做清扫。 从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。但是它有下面三个明显的缺点： 对 CPU 资源敏感； 无法处理浮动垃圾； 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。 G1 收集器 G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征. 被视为 JDK1.7 中 HotSpot 虚拟机的一个重要进化特征。它具备一下特点： 并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。 空间整合：与 CMS 的“标记–清理”算法不同，G1 从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。 可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。 G1 收集器的运作大致分为以下几个步骤： 初始标记 并发标记 最终标记 筛选回收 G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来)。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。 参考javaguide]]></content>
  </entry>
  <entry>
    <title><![CDATA[JVM]]></title>
    <url>%2F2020%2F03%2F19%2FJVM%2F</url>
    <content type="text"><![CDATA[JVM(java virtual machine) JVM的类加载机制 &amp;&amp; 类加载过程 Class 文件需要加载到虚拟机中之后才能运行和使用，那么虚拟机是如何加载这些 Class 文件呢？ 加载 通过全类名获取定义此类的二进制字节流(如硬盘——&gt;内存) 将字节流所代表的静态存储结构转换为方法区的运行时数据结构 在内存中生成一个代表该类的Class 对象,作为方法区这些数据的访问入口 连接（Linking） 验证 主要用于确保Class文件的字节流中包含信息符合JVM要求 准备 主要工作实在方法区中为类变量分配内存空间，并设置该类变量的初始默认值，即零值。如int : 0, float: 0.0f 注意：声明为final类型的变量，在准备阶段会为该变量赋值。 如： public static final int value=100; 准备阶段将为value赋值为100. 解析 JVM会将常量池中的符号引用替换为直接引用。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符7类符号引用进行。 符号引用就是一组符号来描述目标，可以是任何字面量。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。 初始化 初始化阶段就是执行类构造器方法( ) 的过程（不同于类的构造器） 此方法不需定义，是javac编译器自动收集类中静态代码块和类静态变量的赋值操作组成的。 如果没有上述两个。则不会有( )方法调用 JVM规定，只有父类的方法都执行成功后，子类中的该方法才可以被执行。 对于方法的调用，虚拟机会自己确保其在多线程环境中的安全性。因为方法是带锁线程安全，所以在多线程环境下进行类初始化的话可能会引起死锁，并且这种死锁很难被发现。 卸载 JVM的运行时内存 程序计数器 内存空间小，线程私有。用于存储当前运行线程所执行的字节码的行号指示器。 如果正在执行的是 Native 方法，这个计数器的值则为 (Undefined)。此内存区域是唯一一个在 Java 虚拟机规范中没有规定任何 OutOfMemoryError 情况的区域。 方法区（永久代） 属于线程共享内存区域，存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 方法区和永久代的关系很像 Java 中接口和类的关系，类实现了接口，而永久代就是 HotSpot 虚拟机对虚拟机规范中方法区的一种实现方式。 也就是说，永久代是 HotSpot 的概念，方法区是 Java 虚拟机规范中的定义，是一种规范，而永久代是一种实现，一个是标准一个是实现。 JDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 元空间的优点：整个永久代有一个 JVM 本身设置固定大小上限，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小。 运行时常量池运行时常量池是方法区的一部分，用于存放编译期生成的各种字面量和符号引用。 字面量：文本字符串，被声明为final的常量值，基本数据类型的值，其它。 符号应用：类和结构的完全限定名，字段名称和描述符，方法名称和描述符。 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 错误。 JDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 虚拟机栈 线程私有，生命周期和线程一致。描述的是 Java 方法执行的内存模型：每个方法在执行时都会床创建一个栈帧(Stack Frame)用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行结束，就对应着一个栈帧从虚拟机栈中入栈到出栈的过程。 局部变量表：存放了编译期可知的各种基本类型(boolean、byte、char、short、int、float、long、double)、对象引用(reference 类型)和 returnAddress 类型(指向了一条字节码指令的地址) StackOverFlowError： 若 Java 虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 错误。 OutOfMemoryError： 若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出 OutOfMemoryError 错误。 堆 Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap） 本地方法栈 和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种错误。 类加载器 JVM 中内置了三个重要的 ClassLoader，除了 BootstrapClassLoader 其他类加载器均由 Java 实现且全部继承自java.lang.ClassLoader： BootstrapClassLoader(启动类加载器) ：最顶层的加载类，由C++实现，负责加载 JAVA_HOME/lib目录下的jar包和类或者或被 -Xbootclasspath参数指定的路径中的所有类。（String类等java核心类库都是使用引导类加载器） ExtensionClassLoader(扩展类加载器) ：主要负责加载目录 JRE_HOME/lib/ext 目录下的jar包和类，或被 java.ext.dirs 系统变量所指定的路径下的jar包。 AppClassLoader(应用程序类加载器) :面向我们用户的加载器，负责加载当前应用classpath下的所有jar包和类。（用户自定义类：默认使用系统类加载器） 除了上述3种类加载器，我们也可以通过继承java.lang.ClassLoader实现自定义类加载器。 为什么要自定义类加载器？ 答：隔离加载类（不同中间件用不同加载器，避免冲突）；修改类加载方式； 扩展加载源；防止源码泄露。 双亲委派机制 双亲委派机制的核心是保障类的唯一性和安全性。 优点： 1.避免类的重复加载。 2.保护程序安全，防止核心API被篡改。（如，自定义类java.lang.String ,他并不会加载这个。因为他会向上委托到引导类加载器，然后加载那个String。自定义java.lang.Str321 也会报错。禁止的包名，防止其破坏引导类加载器） 对象创建过程 类加载检查 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 分配内存 在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是”标记-清除”，还是”标记-整理”（也称作”标记-压缩”），值得注意的是，复制算法内存也是规整的 内存分配并发问题（补充内容，需要掌握） 在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全： CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。 TLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配 初始化零值 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 设置对象头 初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 执行init方法 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，&lt;init&gt; 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 &lt;init&gt; 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 对象的访问定位 建立对象就是为了使用对象，我们的 Java 程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式由虚拟机实现而定，目前主流的访问方式有①使用句柄和②直接指针两种： 句柄： 如果使用句柄的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息； 直接指针： 如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而 reference 中存储的直接就是对象的地址 这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。 String 对象的两种创建方式 12345String str1 = &quot;abcd&quot;;//先检查字符串常量池中有没有&quot;abcd&quot;，如果字符串常量池中没有，则创建一个，然后 str1 指向字符串常量池中的对象，如果有，则直接将 str1 指向&quot;abcd&quot;&quot;；String str2 = new String(&quot;abcd&quot;);//堆中创建一个新的对象String str3 = new String(&quot;abcd&quot;);//堆中创建一个新的对象System.out.println(str1==str2);//falseSystem.out.println(str2==str3);//false 这两种不同的创建方法是有差别的。 第一种方式是在常量池中拿对象； 第二种方式是直接在堆内存空间创建一个新的对象。 记住一点：只要使用 new 方法，便需要创建新的对象。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mysql回顾]]></title>
    <url>%2F2020%2F03%2F13%2FMysql%E5%9B%9E%E9%A1%BE%2F</url>
    <content type="text"><![CDATA[事务：指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。 ACID 1.原子性（Atomicity）要么都成功，要么都失败,即事务是最小单位，不允许切割 eg: A账户给B账户转账，A扣100元钱，B增加100元钱 2.一致性（Consistency） 数据库在事务执行前后都保持一致性状态。在一致性状态下，所有事务对同一个数据的读取结果都是相同的。 3.隔离性（Isolation） 一个事务所做的修改在最终提交以前，对其它事务是不可见的。 4.持久性（Durability） 一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。 并发事务带来的问题： 1.脏读 T1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。 2.丢失修改 T1，T2,读取到某数据20，T1修改20-1，T2修改20-1，最终19，T1修改被丢失。 3.不可重复读 T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。 4.幻读 同不可重复读一样，T2可能增加了几行，T1蒙蔽了，出现了幻觉。 事务的隔离级别 1.READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 2.READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。** 3.REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 explain:在可重复读中，该sql第一次读取到数据后，就将这些数据加锁（悲观锁），其它事务无法修改这些数据，就可以实现可重复读了。但这种方法却无法锁住insert的数据，所以当事务A先前读取了数据，或者修改了全部数据，事务B还是可以insert数据提交，这时事务A就会发现莫名其妙多了一条之前没有的数据，这就是幻读，不能通过行锁来避免。需要Serializable隔离级别 ，读用读锁，写用写锁，读锁和写锁互斥，这么做可以有效的避免幻读、不可重复读、脏读等问题，但会极大的降低数据库的并发能力。 4.SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读） 锁机制与InnoDB锁算法MyISAM和InnoDB存储引擎使用的锁： MyISAM采用表级锁(table-level locking)。 InnoDB支持行级锁(row-level locking)和表级锁,默认为行级锁 表级锁和行级锁对比： 表级锁： MySQL中锁定 粒度最大 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM和 InnoDB引擎都支持表级锁。 行级锁： MySQL中锁定 粒度最小 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁（why?，如图所示）。 表锁不会发生这种情况，因为表锁是一次性获取所有表的锁，才开始事务。 大表优化当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下： 1. 限定数据的范围务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内； 2. 读/写分离经典的数据库拆分方案，主库负责写，从库负责读； 3. 垂直分区根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。 简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。 如下图所示，这样来说大家应该就更容易理解了。 垂直拆分的优点： 可以使得列数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。 垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂； [ 4. 水平分区保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。 水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。 水平拆分可以支持非常大的数据量。需要注意的一点是：分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 水平拆分最好分库 。 索引Mysql索引主要使用的两种数据结构 哈希索引 O(1) 不适合范围查询。如SELECT * FROM tb1 WHERE id &lt; 500，查500次？ InnoDB不支持哈希索引 对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。 BTree索引 和 B+Tree索引 1.B树的所有节点既存放 键(key) 也存放 数据(data);而B+树只有叶子节点存放 key 和 data，其他内节点只存放key。 2.B树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。 3.B树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显（大大提升范围查询的效率）。 B+树]]></content>
  </entry>
  <entry>
    <title><![CDATA[相册更新]]></title>
    <url>%2F2019%2F12%2F01%2F%E7%9B%B8%E5%86%8C%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[将图片放入photos文件夹下 运行tool.py将相册上传至七牛云photov1 hexo clean hexo d -g 注意：每30天需要将photov1备份到一个新的存储空间，获得30天域名后，修改source/js/src/photo.js的域名]]></content>
  </entry>
  <entry>
    <title><![CDATA[Unet]]></title>
    <url>%2F2019%2F09%2F25%2FUnet%2F</url>
    <content type="text"><![CDATA[论文题目：U-Net: Convolutional Networks for Biomedical Image Segmentation 下载：点击下载 Introduction: 作者提出一个基于FCN改进的U型结构的网络（U-net）和一个依赖strong use of data augmentation的训练策略，可以更充分利用训练样本。 相较于FCN的改进： 1.可以在更少的图片上训练 2.有更精确的分割 与FCN逐点相加不同，U-Net采用将特征在channel维度拼接在一起，形成更“厚”的特征 注：直接复制过来再裁剪到与上采样图片一样大小 该方法允许任意大图片的无缝分割通过一个overlap-tile策略。为了预测框中图像，缺失区域通过镜像输入图像扩张。这种tiling方法对于应用网络到大图像很重要，因为否则结果会被gpu内存限制。为了预测黄色区域的分割，需要蓝色区域作为输入。 数据增强：elastic deformations (弹性形变) 数据增强在训练样本比较少的时候,能够让神经网络学习一些不变性，弹性变换是本文使用的方法。（因为弹性形变是实际细胞中比较常见的一种形变，如果我们能采取数据增强的算法去使网络学习这种形变的不变性，就可以在分割数据集很小的情况下，使网络具有遇见弹性形变还是可以准确的检测出，相当于就是把原图，做了下弹性变形，然后，就相当于扩大了数据集嘛，自然网络就能适应这种弹性变化了，在遇见弹性变形的时候一样可以正确的分类分割） 增加touching cell之间border的权重， 参考： https://blog.csdn.net/jianyuchen23/article/details/79349694]]></content>
  </entry>
  <entry>
    <title><![CDATA[全卷积网络]]></title>
    <url>%2F2019%2F09%2F23%2F%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[论文题目：Fully Convolutional Networks for Semantic Segmentation论文下载: 点击下载 Intorduction: FCN毫无疑问是语义分割领域的经典之作，在FCN出现之前，传统的CNN分割是将像素周围一个小区域作为CNN输入，做训练和预测，这样低效且不准确（忽略整体信息）。FCN主要有三点创新： 卷积化：即将传统CNN结构（文中提到的Alexnet、VGG）最后的全连接层改成卷积层，以便进行直接分割，这是十分有创造性的。这样，使得网络可以接受任意大小的图片，并输出和原图一样大小的分割图。只有这样，才能为每个像素做分类。(像素分类使用图像分类模型（如AlexNet VGGNet等pre-trained model）做迁移学习。) 上采样 or 反卷积：由于网络过程中进行了一系列下采样，使得特征层大小减小，了最后得到的预测层和原图一致，需要采用上采样。 并联跳跃结构：想法类似于resnet和inception，在进行分类预测时利用多层信息。 下图是传统分类CNN和FCN的对比，简单的说，FCN与CNN的区别在于FCN把CNN最后的全连接层换成卷积层，输出一张已经label好的图。 框架如如下，采用了skip connection 上图不够清晰，可以看下图 不同层次对比，其中FCN-8s效果最好 不足：对细节不敏感，没有充分考虑像素之间的关系，缺乏空间一致性。]]></content>
  </entry>
  <entry>
    <title><![CDATA[yolo改进]]></title>
    <url>%2F2019%2F09%2F17%2Fyolo%E6%94%B9%E8%BF%9B%2F</url>
    <content type="text"><![CDATA[论文题目：Assisted Excitation of Activations: A Learning Technique to Improve Object Detectors（CVPR2019）论文下载: 点击下载 摘要：在训练过程中，加入定位信息。可是提升yolov2 map 3.8个点， yolov3 map 2.2个点。这个方法适用于大多数single-stage 目标检测器。只改变了训练过程，推断过程没有任何改变。 Introduction:yolo难以解决得两个痛点：a. difficulty in localization原因：因为yolo同时做分类和定位，最后一层卷积层，更多语义信息，对分类有益。但是spatially course for localization.b. 训练时，前景与背景类别不平衡原因：不同于two-stage 检测器，没有预先减少候选框搜索空间到一个受限制的数目。大多数是简单的负样本。 Related Work: 加入辅助信息到CNN，主要分类两类： 1.同时做检测和分割，提升两个任务的表现。 2.只加入segmentation features来提高检测的精度。 本文提出的方法，在训练检测器时加入weak segmentation ground-truth(即bounding box，从而避免单独引入分割标注，更加简单),并没有增加额外的损失函数。 如上图所示，只在训练时增加了一个Assisted Excitation层。 具体过程： 最终期望的生成特征如下，其中alpha是关于时间的函数用于控制训练中的强度衰减，l+1代表第l+1层，式中c为通道数，e是增强特征： bbox内的像素位置为1，生成一个0-1mask。可见只在bbox内的区域做增强： 增强是按照通道去平均等量加上去的（作者的实验证明该效果最好）： 实验结果： ​ 从上左边的图可以看到，AE强化过的网络有全面的提升，其中在大尺度上的提升更加明显，推测原因是：大物体上加了分割强化后能够获得更强的辨认度，小物体由于本身尺度不大所以增加后也不明显。结果而言印证了这种强化的有效性，但是也完全地陷入了小目标检测的弊端了–像素内容少而被忽视。 ​ 右图的信息不太好辨认。先看yolov2的曲线来说，低iou阈值能够得到更高的改进的精度，说明其召回更好了，但是精度一高就趋于重合，改进失效，说明这种增强提高了低质量bbox的精度。再看yolov3，全IoU都有少量的提高，但是不特别大且没有明显的趋势，说明其采用的多尺度预测能一定程度地解决问题，并在其基础上能对全部精度都有增益。]]></content>
  </entry>
  <entry>
    <title><![CDATA[xml转txt]]></title>
    <url>%2F2019%2F05%2F29%2Fxml%E8%BD%ACtxt%2F</url>
    <content type="text"><![CDATA[先操作一波123456789path = &quot;images/&quot;for filenames in os.walk(pathh): filenames = list(filenames) filenames = filenames[2] for filename in filenames: print(filename) with open (&quot;class_train1.txt&quot;,&apos;a&apos;) as f: f.write(path+filename+&apos;\n&apos;) 再操作一波1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# -*- coding: utf-8 -*-import xml.etree.ElementTree as ETimport pickleimport osfrom os import listdir, getcwdfrom os.path import joinsets = []classes = [&quot;dog&quot;, &quot;person&quot;, &quot;cat&quot;]# 原样保留。size为图片大小# 将ROI的坐标转换为yolo需要的坐标# size是图片的w和h# box里保存的是ROI的坐标（x，y的最大值和最小值）# 返回值为ROI中心点相对于图片大小的比例坐标，和ROI的w、h相对于图片大小的比例def convert(size, box): dw = 1. / (size[0]) dh = 1. / (size[1]) x = (box[0] + box[1]) / 2.0 - 1 y = (box[2] + box[3]) / 2.0 - 1 w = box[1] - box[0] h = box[3] - box[2] x = x * dw w = w * dw y = y * dh h = h * dh return (x, y, w, h)def convert_annotation(image_add): # image_add进来的是带地址的.jpg image_add = os.path.split(image_add)[1] # 截取文件名带后缀 image_add = image_add[0:image_add.find(&apos;.&apos;, 1)] # 删除后缀，现在只有文件名没有后缀 print(image_add) # 现在传进来的只有图片名没有后缀 in_file = open(&apos;xml/&apos; + image_add + &apos;.xml&apos;,encoding=&apos;utf-8&apos;) out_file = open(&apos;hebing2/labels/%s.txt&apos; % (image_add), &apos;w&apos;) tree = ET.parse(in_file) root = tree.getroot() size = root.find(&apos;size&apos;) w = int(size.find(&apos;width&apos;).text) h = int(size.find(&apos;height&apos;).text) # 在一个XML中每个Object的迭代 for obj in root.iter(&apos;object&apos;): # iter()方法可以递归遍历元素/树的所有子元素 # 找到所有的椅子 cls = obj.find(&apos;name&apos;).text # 如果训练标签中的品种不在程序预定品种，或者difficult = 1，跳过此object # cls_id 只等于1 cls_id = 0 xmlbox = obj.find(&apos;bndbox&apos;) # b是每个Object中，一个bndbox上下左右像素的元组 b = (float(xmlbox.find(&apos;xmin&apos;).text), float(xmlbox.find(&apos;xmax&apos;).text), float(xmlbox.find(&apos;ymin&apos;).text), float(xmlbox.find(&apos;ymax&apos;).text)) bb = convert((w, h), b) out_file.write(str(cls_id) + &quot; &quot; + &quot; &quot;.join([str(a) for a in bb]) + &apos;\n&apos;)if not os.path.exists(&apos;hebing2/labels/&apos;): os.makedirs(&apos;hebing2/labels/&apos;)image_adds = open(&quot;class_train1.txt&quot;)for image_add in image_adds: # print(image_add) image_add = image_add.strip() # print (image_add) convert_annotation(image_add) copy from here]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内镜去黑边]]></title>
    <url>%2F2019%2F05%2F09%2F%E5%86%85%E9%95%9C%E5%8E%BB%E9%BB%91%E8%BE%B9%2F</url>
    <content type="text"><![CDATA[对内境图片裁剪，去除黑边，效果如下图所示原图标出矩形框结果123456789101112131415161718192021222324252627282930313233343536373839404142import cv2import osdef main01(): root = &quot;C:\\Users\\liuminggui\\Desktop\\rename\\all\\&quot; # 图片来源路径 save_path = &quot;C:\\Users\\liuminggui\\Desktop\\rename\\save\\&quot; # 图片修改后的保存路径 images = os.listdir(root) for i in images: path = root + i print(i) x,y,w,h=rect_crop(path) # 得到要裁剪的，左上角坐标(x,y)和宽度w,高度h image=cv2.imread(root+i) cv2.imwrite(save_path+i, image[y:y+h+1,x:x+w+1]) # 保存裁剪图片def rect_crop(root=r&apos;F:\Project\DenseBox\data\000001.jpg&apos;): img = cv2.imread(root) # img = cv2.pyrDown(img) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # 得到灰度图 x_,y_ = gray.shape ret,thresh = cv2.threshold(gray,50,255,cv2.THRESH_BINARY) # 测试得到50比较好 # 注意opencv2和3这个函数可能有2个或者三个返回值 img111,contours,hier = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE) for c in contours: # 遍历所有轮廓 print(c) x,y,w,h = cv2.boundingRect(c) if w * h &gt; x_ * y_ * 0.33: # 轮廓大于整个面积的1/3，应该就是我们要找的 cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2) print(x,y,w,h) cv2.imshow(&apos;img+rectangle&apos;,img) cv2.imshow(&apos;thresh&apos;, thresh) cv2.waitKey(0) return (x,y,w,h) return (0,0,0,0) # 找不到大矩形轮廓，则返回默认值0if __name__ == &apos;__main__&apos;: # main1() # liuminggui() main01() cover from YSH and sloan]]></content>
      <categories>
        <category>code</category>
      </categories>
      <tags>
        <tag>code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日本胃癌论文总结2]]></title>
    <url>%2F2019%2F05%2F07%2F%E6%97%A5%E6%9C%AC%E8%83%83%E7%99%8C%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%932%2F</url>
    <content type="text"><![CDATA[论文题目：Automatic detection of early gastric cancer in endoscopic images using a transferring convolutional neural network 摘要： Accuracy :87.6% heat map accuracy: 82.8%网络：GoogLeNet, 22 conv layers, pretrained on ImageNet 方法： 数据集处理 CNN迁移学习 judge normal vs cancer visualization – heat map 数据集处理（most important）总共有926张分辨率为1000*870的图片其中228包含胃癌 训练数据：从228张选出100张，然后对这100张，每张随机裁剪出100张左右224224的胃癌图片，每张都要包含80%病变区域，得到9587张224224的胃癌图片从包含胃癌和不包含胃癌的图片中随机裁剪出9800张224*224不包含胃癌的正常图片 测试数据：在训练数据裁剪未使用的包含和不包含胃癌的图片中，裁剪出4653胃癌图片和4997正常图片 结果]]></content>
  </entry>
  <entry>
    <title><![CDATA[日本胃癌论文总结1]]></title>
    <url>%2F2019%2F05%2F05%2F%E6%97%A5%E6%9C%AC%E8%83%83%E7%99%8C%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%931%2F</url>
    <content type="text"><![CDATA[论文题目：Application of artificial intelligence using a convolutional neural network for detecting gastric cancer in endoscopic images论文下载: 点击下载 测试集：13584张胃癌图片，包含2639个胃癌病变（经组织学验证）测试集： 2296张胃癌图片，包含69个病人，77个胃癌病变(62 cases had 1 gastric cancer lesion, 6 had 2 lesions, and 1 had 3 lesions)，每个病人18~69张图片。速度： 共用49s 检测2296张图片overall sensitivity： 92.2% （71/77） 71个胃癌病变成功被检测出来positive predictive value： 30.6%=71/（71+161） 161个非癌性病变误检测，过半误检测为胃炎 结果：实验：将训练集resize到300*300,送入网络fine-tune参数，然后检测测试集，检测胃癌病变区域。将其用矩形框框出。 714张图片被诊断出胃癌 714/2639=31.1% 测试集中52个（67.5%）是早期胃癌T1, 25个(32.5%)是advanced cancer T2,T3,T4 平均肿瘤大小是24mm(3到170mm) 思考：准确率该如何计算，如果单看被检测所有的测试集图片，只有不到1/3的图片被检测出有胃癌。但是如果按照检测的胃癌病变，一共有71/77个病变被检测出来！我想了下，主要是因为一个病变包含多张图片，作者认为只要某个病变的一张图片被正确诊断，就认为该病变被成功检测出来。类似于多示例学习。但是这样的话，误诊的也很高，这个161个非癌性病变被误诊是怎么算出来的？？？医生对误诊的区域进行手动分类统计？？？]]></content>
  </entry>
  <entry>
    <title><![CDATA[ObjectDetection]]></title>
    <url>%2F2019%2F04%2F17%2FObjectDetection%2F</url>
    <content type="text"><![CDATA[1.Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.pdf点击下载2.Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf点击下载3.yolov3点击下载]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch]]></title>
    <url>%2F2019%2F04%2F11%2Fpytorch%2F</url>
    <content type="text"><![CDATA[损失函数1. CrossEntropylossa.交叉熵损失函数，常用于分类b.用这个loss前面不需要加 softmax层c.该函数限制了target的类型为torch.LongTensor1234567891011import torch as tfrom torch import nnfrom torch.autograd import Variable as V# batch_size=4, 计算每个类别分数（二分类）output = V(t.randn(4,2)) # batch_size * C=(batch_size, C)# target必须是LongTensor!target =V(t.Tensor([1,0,1,1])).long()criterion = nn.CrossEntropyLoss()loss = criterion(output, target)print(&apos;loss&apos;, loss) output: loss tensor(1.0643) 2. toch.nn.MSELoss均方损失函数，类似于nn.L1Loss函数：1234567import torchloss_fn = torch.nn.MSELoss(reduce=False, size_average=False)input = torch.autograd.Variable(torch.randn(3,4))target = torch.autograd.Variable(torch.randn(3,4))loss = loss_fn(input, target)print(input); print(target); print(loss)print(input.size(), target.size(), loss.size()) output:]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>summary</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用总结]]></title>
    <url>%2F2019%2F04%2F10%2F%E5%B8%B8%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[ROC曲线根据机器学习中分类器的预测得分对样例(每个样例的阳性概率)进行排序，按照顺序逐个把样本的概率作为阈值thresholds进行预测，计算出FPR和TPR。分别以FPR、TPR为横纵坐标作图即可得到ROC曲线。所以作ROC曲线时，需要先求出FPR和TPR。这两个变量的定义：FPR = TP/(TP+FN) TPR = TP/(TP+FP) 将样本输入分类器，每个样本将得到一个预测得分。我们通过设置不同的截断点，即可截取不同的信息。对应此示例图中，每个阈值的识别结果对应一个点(FPR，TPR)。当阈值取最大时，所有样本都被识别成负样本，对应于坐下角的点(0,0); 当阈值取最小时，所有样本都被识别成正样本，对应于右上角的点(1,1)，随着阈值从最大变化到最小，TP和FP都逐渐大；python中调用ROC12345678910111213141516171819import numpy as npfrom sklearn import metricsimport matplotlib.pyplot as pltfrom sklearn.metrics import auc# 真实标签y_true = np.array([0,0,1,1])print(&apos;y_true: &apos;, y_true)# y_score为预测为阳性的得分（说概率不大准确，因为这个score可以大于1）y_score = np.array([0.1, 0.35, 0.3, 0.8])print(&apos;y_score:&apos;, y_score)fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score, pos_label=1)print(&apos;fpr&apos;, fpr)print(&apos;tpr&apos;, tpr)print(&apos;thresholds&apos;, thresholds)plt.plot(fpr,tpr,marker = &apos;o&apos;)plt.show()AUC = auc(fpr, tpr)print(&apos;AUC&apos;, AUC) 输出：阈值[0]表示没有被预测的实例，并且被任意设置为max(y_score) + 1 极大似然估计]]></content>
      <categories>
        <category>一些总结</category>
      </categories>
      <tags>
        <tag>summary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yolo 理论总结]]></title>
    <url>%2F2019%2F04%2F06%2Fyolo%E7%90%86%E8%AE%BA%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[对象识别和定位，可以看成两个任务：找到图片中某个存在对象的区域，然后识别出该区域中具体是哪个对象。 对象识别这件事（一张图片仅包含一个对象，且基本占据图片的整个范围），最近几年基于CNN卷积神经网络的各种方法已经能达到不错的效果了。所以主要需要解决的问题是，对象在哪里。 最简单的想法，就是遍历图片中所有可能的位置，地毯式搜索不同大小，不同宽高比，不同位置的每个区域，逐一检测其中是否存在某个对象，挑选其中概率最大的结果作为输出。显然这种方法效率太低。 RCNN提出候选区(Region Proposals)的方法，先从图片中搜索出一些可能存在对象的候选区（Selective Search），然后对每个候选区进行对象识别。大幅提升了对象识别和定位的效率。总体来说，RCNN系列依然是两阶段处理模式：先提出候选区，再识别候选区中的对象。 yolov1 yolov1详解(非常详细，推荐) 补充：边框回归：对于窗口一般使用四维向量(x,y,w,h)来表示， 分别表示窗口的中心点坐标和宽高。 对于图 2, 红色的框 P 代表原始的Proposal, 绿色的框 G 代表目标的 Ground Truth， 我们的目标是寻找一种关系使得输入原始的窗口 P 经过映射得到一个跟真实窗口 G 更接近的回归窗口G^。 YOLOV1的bounding box并不是Faster RCNN的AnchorFaster RCNN等一些算法采用每个grid中手工设置n个Anchor（先验框，预先设置好位置的bounding box）的设计，每个Anchor有不同的大小和宽高比。YOLO的bounding box看起来很像一个grid中2个Anchor，但它们不是。YOLO并没有预先设置2个bounding box的大小和形状，也没有对每个bounding box分别输出一个对象的预测。它的意思仅仅是对一个对象预测出2个bounding box，选择预测得相对比较准的那个。 Yolov2 yolov2改变：batch normalization,采用了anchor,借鉴Faster RCNN的做法，YOLO2也尝试采用先验框（anchor）。在每个grid预先设定一组不同大小和宽高比的边框，来覆盖整个图像的不同位置和多种尺度，这些先验框作为预定义的候选区在神经网络中将检测其中是否存在对象，以及微调边框的位置。]]></content>
      <categories>
        <category>yolo</category>
      </categories>
      <tags>
        <tag>summary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[斯坦福皮肤癌论文总结]]></title>
    <url>%2F2019%2F03%2F15%2F%E6%96%AF%E5%9D%A6%E7%A6%8F%E7%9A%AE%E8%82%A4%E7%99%8C%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Dermatologist-level classification of skin cancer with deep neural networks背景 以往的皮肤癌分类器往往缺乏好的泛化能力，由于缺少数据和focus on 标准任务，如只对专用医学设备产生的图片进行分类。无法对如手机拍摄的等因为缩放，角度，光线问题的照片进行分类。该文提出一种端对端的CNN，对皮肤癌进行分类。可以达到专家水平甚至更好。 数据 用了129450张图像（比以往的数据集大两个数量级）包含2032种不同的疾病。测试数据是由21位皮肤科专家标注的。 将数据划分： 127,463用于训练和validation 1,942 biopsy-labelled（活检）用于测试 模型GoogLeNet Inception V3 (用2014ImageNet预训练，1.28 million images) 结果蓝色的是CNN,红色的点代表皮肤病专家，绿色的是皮肤病专家的平均水平，可以看出，CNN胜出 在first level nodes（benign lesions, malignant lesions and non-neoplastic lesions)3 class partision 任务中可以达到72.1%的平均准确率，两个皮肤科专家分别达到65.56%和66.0%其次，在second level nodes（9分类）中CNN可以达到55.4%，两个专家分别是53.3和55.0可以看出，用更好的疾病划分方法可以提高准确率 亮点1.一种给疾病分类的算法充分利用如下疾病的树状图分类，好像这个Partition Algorithm 挺好使的看以上结果的时候可以发现，有PA和没有PA，可以提升好几个点，下图是PA具体算法2.本文的训练数据比以往大了两个数量级，数据为王。3.不仅用了专业医学设备产生的图片4.展望手机app端，提升逼格]]></content>
  </entry>
  <entry>
    <title><![CDATA[组会ppt]]></title>
    <url>%2F2019%2F03%2F14%2F%E7%BB%84%E4%BC%9Appt%2F</url>
    <content type="text"><![CDATA[1.2018.3.8 early gastric cancer.ppt点击下载 论文题目：Automatic detection of early gastric cancer in endoscopic images点击下载]]></content>
      <categories>
        <category>PPT</category>
      </categories>
      <tags>
        <tag>PPT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIApaper]]></title>
    <url>%2F2018%2F12%2F10%2FMIApaper%2F</url>
    <content type="text"><![CDATA[1.一种基于原型学习的多示例卷积神经网络点击下载2.Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification点击下载3.2018.12.12小组会PPT点击下载4.自然语言处理paper reading点击下载5.Prototypical Networks for Few-shot Learning点击下载6.Automatic detection of early gastric cancer in endoscopic images点击下载7.what is this点击下载8.Matching Network点击下载9.斯坦福皮肤癌点击下载10.PathologicalEvidenceExplorationinDeepRetinalImageDiagnosis点击下载11.胃癌+AI整理点击下载]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paper]]></title>
    <url>%2F2018%2F11%2F30%2Fpaper%2F</url>
    <content type="text"><![CDATA[paper reading论文下载]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2018%2F11%2F30%2Ftest%2F</url>
    <content type="text"><![CDATA[this is fucking crazy 你好 今天是周五 明天放假了 还有好多作业 this is a test今天是个好日子]]></content>
  </entry>
</search>
